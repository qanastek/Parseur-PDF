<article><preamble>Alexandrov_2015_A_Modified_Tripartite_Model_for_Document_Representation_in_Internet_Sociology</preamble><titre>A Modified Tripartite Model for Document</titre><auteur>Unknown</auteur><abstract> Seven years ago Peter Mika (Yahoo! Research) proposed a tripartite model of actors, concepts and instances for document representation in the study of social networks. We propose a modified model, where instead of document authors we consider textual mentions of persons and institutions as actors. This representation proves to be more appropriate for the solution of a range of Internet Sociology tasks. In the paper we describe experiments with the modified model and provide some background on the tools that can be used to build it. The model is tested on the experimental corpora of Russian news (educational domain). The research reflects the pilot study findings. </abstract><biblio /></article>
<article><preamble>Doyle_2005_Automatic_Categorization_of_Author_Gender</preamble><titre>Automatic Categorization of Author Gender</titre><auteur>Unknown</auteur><abstract> We present a method for automatic categorization of author gender via n-gram analysis. Using a corpus of British student essays, experiments using character-level, wordlevel, and part-of-speech n-grams are performed. The peak accuracy for all methods is roughly equal, reaching a maximum of 81%. These results are on par with other, established techniques, while retaining the simplicity and ease-of-generalization inherent in n-gram techniques.</abstract><biblio /></article>
<article><preamble>Alexandrov_2015_A_Modified_Tripartite_Model_for_Document_Representation_in_Internet_Sociology</preamble><titre>A Modified Tripartite Model for Document</titre><auteur>Unknown</auteur><abstract> Seven years ago Peter Mika (Yahoo! Research) proposed a tripartite model of actors, concepts and instances for document representation in the study of social networks. We propose a modified model, where instead of document authors we consider textual mentions of persons and institutions as actors. This representation proves to be more appropriate for the solution of a range of Internet Sociology tasks. In the paper we describe experiments with the modified model and provide some background on the tools that can be used to build it. The model is tested on the experimental corpora of Russian news (educational domain). The research reflects the pilot study findings. </abstract><biblio /></article>
<article><preamble>Doyle_2005_Automatic_Categorization_of_Author_Gender</preamble><titre>Automatic Categorization of Author Gender</titre><auteur>Unknown</auteur><abstract> We present a method for automatic categorization of author gender via n-gram analysis. Using a corpus of British student essays, experiments using character-level, wordlevel, and part-of-speech n-grams are performed. The peak accuracy for all methods is roughly equal, reaching a maximum of 81%. These results are on par with other, established techniques, while retaining the simplicity and ease-of-generalization inherent in n-gram techniques.</abstract><biblio /></article>
<article><preamble>Furui_2004_Speech-to-text_and_speech-to-speech_summarization_of_spontaneous_speech</preamble><titre>IEEE TRANSACTIONS ON SPEECH AND AUDIO PROCESSING, VOL. 12, NO. 4, JULY 2004</titre><auteur>Unknown</auteur><abstract>This paper presents techniques for speech-to-text and speech-to-speech automatic summarization based on speech unit extraction and concatenation. For the former case, a two-stage summarization method consisting of important sentence extraction and word-based sentence compaction is investigated. Sentence and word units which maximize the weighted sum of linguistic likelihood, amount of information, confidence measure, and grammatical likelihood of concatenated units are extracted from the speech recognition results and concatenated for producing summaries. For the latter case, sentences, words, and between-filler units are investigated as units to be extracted from original speech. These methods are applied to the summarization of unrestricted-domain spontaneous presentations and evaluated by objective and subjective measures. It was confirmed that proposed methods are effective in spontaneous speech summarization. Index Terms&#8212;Presentation, speech recognition, speech summarization, speech-to-speech, speech-to-text, spontaneous speech.</abstract><biblio /></article>
<article><preamble>Gonzalez_2018_Automated_Sentence_Boundary_Detection_in_Modern_StandardArabic_Transcripts_using_Deep_Neural_Networks</preamble><titre>Available</titre><auteur>Unknown</auteur><abstract> </abstract><biblio /></article>
<article><preamble>Levner_2007_Fuzzifying_clustering_algorithms_The_case_study_of_MajorClust</preamble><titre>Fuzzifying clustering algorithms:</titre><auteur>Unknown</auteur><abstract> Among various document clustering algorithms that have been proposed so far, the most useful are those that automatically reveal the number of clusters and assign each target document to exactly one cluster. However, in many real situations, there not exists an exact boundary between different clusters. In this work, we introduce a fuzzy version of the MajorClust algorithm. The proposed clustering method assigns documents to more than one category by taking into account a membership function for both, edges and nodes of the corresponding underlying graph. Thus, the clustering problem is formulated in terms of weighted fuzzy graphs. The fuzzy approach permits to decrease some negative effects which appear in clustering of large-sized corpora with noisy data.</abstract><biblio /></article>
<article><preamble>Lin_2004_Rouge</preamble><titre>ROUGE: A Package for Automatic Evaluation of Summaries</titre><auteur>Unknown</auteur><abstract> ROUGE stands for Recall-Oriented Understudy for Gisting Evaluation. It includes measures to automatically determine the quality of a summary by comparing it to other (ideal) summaries created by humans. The measures count the number of overlapping units such as n-gram, word sequences, and word pairs between the computer-generated summary to be evaluated and the ideal summaries created by humans. This paper introduces four different ROUGE measures: ROUGE-N, ROUGE-L, ROUGE-W, and ROUGE-S included in the ROUGE summarization evaluation package and their evaluatio ns. Three of them have been used in the Document Understanding Conference (DUC) 2004, a large-scale summarization evaluation sponsored by NIST. 1</abstract><biblio /></article>
<article><preamble>Metsis_2006_Spam_filtering_with_naive_bayes-which_naive_bayes</preamble><titre>Spam Filtering with Naive Bayes &#8211; Which Naive Bayes?</titre><auteur>Unknown</auteur><abstract> Naive Bayes is very popular in commercial and open-source anti-spam e-mail filters. There are, however, several forms of Naive Bayes, something the anti-spam literature does not always acknowledge. We discuss five different versions of Naive Bayes, and compare them on six new, non-encoded datasets, that contain ham messages of particular Enron users and fresh spam messages. The new datasets, which we make publicly available, are more realistic than previous comparable benchmarks, because they maintain the temporal order of the messages in the two categories, and they emulate the varying proportion of spam and ham messages that users receive over time. We adopt an experimental procedure that emulates the incremental training of personalized spam filters, and we plot roc curves that allow us to compare the different versions of nb over the entire tradeoff between true positives and true negatives.</abstract><biblio /></article>
<article><preamble>Mikolov_2013_Distributed_representations_of_words_and_phrases_and_their_compositionality</preamble><titre>Distributed Representations of Words and Phrases</titre><auteur>Unknown</auteur><abstract> The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of &#8220;Canada&#8221; and &#8220;Air&#8221; cannot be easily combined to obtain &#8220;Air Canada&#8221;. Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.</abstract><biblio /></article>
<article><preamble>Torres-Moreno_2012_Artex_is_another_text_summarizer</preamble><titre>Artex is AnotheR TEXt summarizer</titre><auteur>Unknown</auteur><abstract> This paper describes Artex, another algorithm for Automatic Text Summarization. In order to rank sentences, a simple inner product is calculated between each sentence, a document vector (text topic) and a lexical vector (vocabulary used by a sentence). Summaries are then generated by assembling the highest ranked sentences. No ruled-based linguistic post-processing is necessary in order to obtain summaries. Tests over several datasets (coming from Document Understanding Conferences (DUC), Text Analysis Conference (TAC), evaluation campaigns, etc.) in French, English and Spanish have shown that Artex summarizer achieves interesting results.</abstract><biblio /></article>
<article><preamble>Vanderwende_2007_Beyond_SumBasic</preamble><titre>Information Processing and Management 43 (2007) 1606&#8211;1618</titre><auteur>Unknown</auteur><abstract> In recent years, there has been increased interest in topic-focused multi-document summarization. In this task, automatic summaries are produced in response to a speci&#64257;c information request, or topic, stated by the user. The system we have designed to accomplish this task comprises four main components: a generic extractive summarization system, a topic-focusing component, sentence simpli&#64257;cation, and lexical expansion of topic words. This paper details each of these components, together with experiments designed to quantify their individual contributions. We include an analysis of our results on two large datasets commonly used to evaluate task-focused summarization, the DUC2005 and DUC2006 datasets, using automatic metrics. Additionally, we include an analysis of our results on the DUC2006 task according to human evaluation metrics. In the human evaluation of system summaries compared to human summaries, i.e., the Pyramid method, our system ranked &#64257;rst out of 22 systems in terms of overall mean Pyramid score; and in the human evaluation of summary responsiveness to the topic, our system ranked third out of 35 systems. &#211; 2007 Elsevier Ltd. All rights reserved. </abstract><biblio /></article>
<article><preamble>Alexandrov_2015_A_Modified_Tripartite_Model_for_Document_Representation_in_Internet_Sociology</preamble><titre>A Modified Tripartite Model for Document</titre><auteur>Unknown</auteur><abstract> Seven years ago Peter Mika (Yahoo! Research) proposed a tripartite model of actors, concepts and instances for document representation in the study of social networks. We propose a modified model, where instead of document authors we consider textual mentions of persons and institutions as actors. This representation proves to be more appropriate for the solution of a range of Internet Sociology tasks. In the paper we describe experiments with the modified model and provide some background on the tools that can be used to build it. The model is tested on the experimental corpora of Russian news (educational domain). The research reflects the pilot study findings. </abstract><biblio /></article>
<article><preamble>Doyle_2005_Automatic_Categorization_of_Author_Gender</preamble><titre>Automatic Categorization of Author Gender</titre><auteur>Unknown</auteur><abstract> We present a method for automatic categorization of author gender via n-gram analysis. Using a corpus of British student essays, experiments using character-level, wordlevel, and part-of-speech n-grams are performed. The peak accuracy for all methods is roughly equal, reaching a maximum of 81%. These results are on par with other, established techniques, while retaining the simplicity and ease-of-generalization inherent in n-gram techniques.</abstract><biblio /></article>
<article><preamble>Furui_2004_Speech-to-text_and_speech-to-speech_summarization_of_spontaneous_speech</preamble><titre>IEEE TRANSACTIONS ON SPEECH AND AUDIO PROCESSING, VOL. 12, NO. 4, JULY 2004</titre><auteur>Unknown</auteur><abstract>This paper presents techniques for speech-to-text and speech-to-speech automatic summarization based on speech unit extraction and concatenation. For the former case, a two-stage summarization method consisting of important sentence extraction and word-based sentence compaction is investigated. Sentence and word units which maximize the weighted sum of linguistic likelihood, amount of information, confidence measure, and grammatical likelihood of concatenated units are extracted from the speech recognition results and concatenated for producing summaries. For the latter case, sentences, words, and between-filler units are investigated as units to be extracted from original speech. These methods are applied to the summarization of unrestricted-domain spontaneous presentations and evaluated by objective and subjective measures. It was confirmed that proposed methods are effective in spontaneous speech summarization. Index Terms&#8212;Presentation, speech recognition, speech summarization, speech-to-speech, speech-to-text, spontaneous speech.</abstract><biblio /></article>
<article><preamble>Gonzalez_2018_Automated_Sentence_Boundary_Detection_in_Modern_StandardArabic_Transcripts_using_Deep_Neural_Networks</preamble><titre>Available</titre><auteur>Unknown</auteur><abstract> </abstract><biblio /></article>
<article><preamble>Levner_2007_Fuzzifying_clustering_algorithms_The_case_study_of_MajorClust</preamble><titre>Fuzzifying clustering algorithms:</titre><auteur>Unknown</auteur><abstract> Among various document clustering algorithms that have been proposed so far, the most useful are those that automatically reveal the number of clusters and assign each target document to exactly one cluster. However, in many real situations, there not exists an exact boundary between different clusters. In this work, we introduce a fuzzy version of the MajorClust algorithm. The proposed clustering method assigns documents to more than one category by taking into account a membership function for both, edges and nodes of the corresponding underlying graph. Thus, the clustering problem is formulated in terms of weighted fuzzy graphs. The fuzzy approach permits to decrease some negative effects which appear in clustering of large-sized corpora with noisy data.</abstract><biblio /></article>
<article><preamble>Lin_2004_Rouge</preamble><titre>ROUGE: A Package for Automatic Evaluation of Summaries</titre><auteur>Unknown</auteur><abstract> ROUGE stands for Recall-Oriented Understudy for Gisting Evaluation. It includes measures to automatically determine the quality of a summary by comparing it to other (ideal) summaries created by humans. The measures count the number of overlapping units such as n-gram, word sequences, and word pairs between the computer-generated summary to be evaluated and the ideal summaries created by humans. This paper introduces four different ROUGE measures: ROUGE-N, ROUGE-L, ROUGE-W, and ROUGE-S included in the ROUGE summarization evaluation package and their evaluatio ns. Three of them have been used in the Document Understanding Conference (DUC) 2004, a large-scale summarization evaluation sponsored by NIST. 1</abstract><biblio /></article>
<article><preamble>Metsis_2006_Spam_filtering_with_naive_bayes-which_naive_bayes</preamble><titre>Spam Filtering with Naive Bayes &#8211; Which Naive Bayes?</titre><auteur>Unknown</auteur><abstract> Naive Bayes is very popular in commercial and open-source anti-spam e-mail filters. There are, however, several forms of Naive Bayes, something the anti-spam literature does not always acknowledge. We discuss five different versions of Naive Bayes, and compare them on six new, non-encoded datasets, that contain ham messages of particular Enron users and fresh spam messages. The new datasets, which we make publicly available, are more realistic than previous comparable benchmarks, because they maintain the temporal order of the messages in the two categories, and they emulate the varying proportion of spam and ham messages that users receive over time. We adopt an experimental procedure that emulates the incremental training of personalized spam filters, and we plot roc curves that allow us to compare the different versions of nb over the entire tradeoff between true positives and true negatives.</abstract><biblio /></article>
<article><preamble>Mikolov_2013_Distributed_representations_of_words_and_phrases_and_their_compositionality</preamble><titre>Distributed Representations of Words and Phrases</titre><auteur>Unknown</auteur><abstract> The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of &#8220;Canada&#8221; and &#8220;Air&#8221; cannot be easily combined to obtain &#8220;Air Canada&#8221;. Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.</abstract><biblio /></article>
<article><preamble>Torres-Moreno_2012_Artex_is_another_text_summarizer</preamble><titre>Artex is AnotheR TEXt summarizer</titre><auteur>Unknown</auteur><abstract> This paper describes Artex, another algorithm for Automatic Text Summarization. In order to rank sentences, a simple inner product is calculated between each sentence, a document vector (text topic) and a lexical vector (vocabulary used by a sentence). Summaries are then generated by assembling the highest ranked sentences. No ruled-based linguistic post-processing is necessary in order to obtain summaries. Tests over several datasets (coming from Document Understanding Conferences (DUC), Text Analysis Conference (TAC), evaluation campaigns, etc.) in French, English and Spanish have shown that Artex summarizer achieves interesting results.</abstract><biblio /></article>
<article><preamble>Vanderwende_2007_Beyond_SumBasic</preamble><titre>Information Processing and Management 43 (2007) 1606&#8211;1618</titre><auteur>Unknown</auteur><abstract> In recent years, there has been increased interest in topic-focused multi-document summarization. In this task, automatic summaries are produced in response to a speci&#64257;c information request, or topic, stated by the user. The system we have designed to accomplish this task comprises four main components: a generic extractive summarization system, a topic-focusing component, sentence simpli&#64257;cation, and lexical expansion of topic words. This paper details each of these components, together with experiments designed to quantify their individual contributions. We include an analysis of our results on two large datasets commonly used to evaluate task-focused summarization, the DUC2005 and DUC2006 datasets, using automatic metrics. Additionally, we include an analysis of our results on the DUC2006 task according to human evaluation metrics. In the human evaluation of system summaries compared to human summaries, i.e., the Pyramid method, our system ranked &#64257;rst out of 22 systems in terms of overall mean Pyramid score; and in the human evaluation of summary responsiveness to the topic, our system ranked third out of 35 systems. &#211; 2007 Elsevier Ltd. All rights reserved. </abstract><biblio /></article>
<article><preamble>Alexandrov_2015_A_Modified_Tripartite_Model_for_Document_Representation_in_Internet_Sociology</preamble><titre>A Modified Tripartite Model for Document</titre><auteur>Unknown</auteur><abstract> Seven years ago Peter Mika (Yahoo! Research) proposed a tripartite model of actors, concepts and instances for document representation in the study of social networks. We propose a modified model, where instead of document authors we consider textual mentions of persons and institutions as actors. This representation proves to be more appropriate for the solution of a range of Internet Sociology tasks. In the paper we describe experiments with the modified model and provide some background on the tools that can be used to build it. The model is tested on the experimental corpora of Russian news (educational domain). The research reflects the pilot study findings. </abstract><biblio /></article>
<article><preamble>Doyle_2005_Automatic_Categorization_of_Author_Gender</preamble><titre>Automatic Categorization of Author Gender</titre><auteur>Unknown</auteur><abstract> We present a method for automatic categorization of author gender via n-gram analysis. Using a corpus of British student essays, experiments using character-level, wordlevel, and part-of-speech n-grams are performed. The peak accuracy for all methods is roughly equal, reaching a maximum of 81%. These results are on par with other, established techniques, while retaining the simplicity and ease-of-generalization inherent in n-gram techniques.</abstract><biblio /></article>
<article><preamble>Furui_2004_Speech-to-text_and_speech-to-speech_summarization_of_spontaneous_speech</preamble><titre>IEEE TRANSACTIONS ON SPEECH AND AUDIO PROCESSING, VOL. 12, NO. 4, JULY 2004</titre><auteur>Unknown</auteur><abstract>This paper presents techniques for speech-to-text and speech-to-speech automatic summarization based on speech unit extraction and concatenation. For the former case, a two-stage summarization method consisting of important sentence extraction and word-based sentence compaction is investigated. Sentence and word units which maximize the weighted sum of linguistic likelihood, amount of information, confidence measure, and grammatical likelihood of concatenated units are extracted from the speech recognition results and concatenated for producing summaries. For the latter case, sentences, words, and between-filler units are investigated as units to be extracted from original speech. These methods are applied to the summarization of unrestricted-domain spontaneous presentations and evaluated by objective and subjective measures. It was confirmed that proposed methods are effective in spontaneous speech summarization. Index Terms&#8212;Presentation, speech recognition, speech summarization, speech-to-speech, speech-to-text, spontaneous speech.</abstract><biblio /></article>
<article><preamble>Gonzalez_2018_Automated_Sentence_Boundary_Detection_in_Modern_StandardArabic_Transcripts_using_Deep_Neural_Networks</preamble><titre>Available</titre><auteur>Unknown</auteur><abstract> </abstract><biblio /></article>
<article><preamble>Levner_2007_Fuzzifying_clustering_algorithms_The_case_study_of_MajorClust</preamble><titre>Fuzzifying clustering algorithms:</titre><auteur>Unknown</auteur><abstract> Among various document clustering algorithms that have been proposed so far, the most useful are those that automatically reveal the number of clusters and assign each target document to exactly one cluster. However, in many real situations, there not exists an exact boundary between different clusters. In this work, we introduce a fuzzy version of the MajorClust algorithm. The proposed clustering method assigns documents to more than one category by taking into account a membership function for both, edges and nodes of the corresponding underlying graph. Thus, the clustering problem is formulated in terms of weighted fuzzy graphs. The fuzzy approach permits to decrease some negative effects which appear in clustering of large-sized corpora with noisy data.</abstract><biblio /></article>
<article><preamble>Lin_2004_Rouge</preamble><titre>ROUGE: A Package for Automatic Evaluation of Summaries</titre><auteur>Unknown</auteur><abstract> ROUGE stands for Recall-Oriented Understudy for Gisting Evaluation. It includes measures to automatically determine the quality of a summary by comparing it to other (ideal) summaries created by humans. The measures count the number of overlapping units such as n-gram, word sequences, and word pairs between the computer-generated summary to be evaluated and the ideal summaries created by humans. This paper introduces four different ROUGE measures: ROUGE-N, ROUGE-L, ROUGE-W, and ROUGE-S included in the ROUGE summarization evaluation package and their evaluatio ns. Three of them have been used in the Document Understanding Conference (DUC) 2004, a large-scale summarization evaluation sponsored by NIST. 1</abstract><biblio /></article>
<article><preamble>Metsis_2006_Spam_filtering_with_naive_bayes-which_naive_bayes</preamble><titre>Spam Filtering with Naive Bayes &#8211; Which Naive Bayes?</titre><auteur>Unknown</auteur><abstract> Naive Bayes is very popular in commercial and open-source anti-spam e-mail filters. There are, however, several forms of Naive Bayes, something the anti-spam literature does not always acknowledge. We discuss five different versions of Naive Bayes, and compare them on six new, non-encoded datasets, that contain ham messages of particular Enron users and fresh spam messages. The new datasets, which we make publicly available, are more realistic than previous comparable benchmarks, because they maintain the temporal order of the messages in the two categories, and they emulate the varying proportion of spam and ham messages that users receive over time. We adopt an experimental procedure that emulates the incremental training of personalized spam filters, and we plot roc curves that allow us to compare the different versions of nb over the entire tradeoff between true positives and true negatives.</abstract><biblio /></article>
<article><preamble>Mikolov_2013_Distributed_representations_of_words_and_phrases_and_their_compositionality</preamble><titre>Distributed Representations of Words and Phrases</titre><auteur>Unknown</auteur><abstract> The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of &#8220;Canada&#8221; and &#8220;Air&#8221; cannot be easily combined to obtain &#8220;Air Canada&#8221;. Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.</abstract><biblio /></article>
<article><preamble>Torres-Moreno_2012_Artex_is_another_text_summarizer</preamble><titre>Artex is AnotheR TEXt summarizer</titre><auteur>Unknown</auteur><abstract> This paper describes Artex, another algorithm for Automatic Text Summarization. In order to rank sentences, a simple inner product is calculated between each sentence, a document vector (text topic) and a lexical vector (vocabulary used by a sentence). Summaries are then generated by assembling the highest ranked sentences. No ruled-based linguistic post-processing is necessary in order to obtain summaries. Tests over several datasets (coming from Document Understanding Conferences (DUC), Text Analysis Conference (TAC), evaluation campaigns, etc.) in French, English and Spanish have shown that Artex summarizer achieves interesting results.</abstract><biblio /></article>
<article><preamble>Vanderwende_2007_Beyond_SumBasic</preamble><titre>Information Processing and Management 43 (2007) 1606&#8211;1618</titre><auteur>Unknown</auteur><abstract> In recent years, there has been increased interest in topic-focused multi-document summarization. In this task, automatic summaries are produced in response to a speci&#64257;c information request, or topic, stated by the user. The system we have designed to accomplish this task comprises four main components: a generic extractive summarization system, a topic-focusing component, sentence simpli&#64257;cation, and lexical expansion of topic words. This paper details each of these components, together with experiments designed to quantify their individual contributions. We include an analysis of our results on two large datasets commonly used to evaluate task-focused summarization, the DUC2005 and DUC2006 datasets, using automatic metrics. Additionally, we include an analysis of our results on the DUC2006 task according to human evaluation metrics. In the human evaluation of system summaries compared to human summaries, i.e., the Pyramid method, our system ranked &#64257;rst out of 22 systems in terms of overall mean Pyramid score; and in the human evaluation of summary responsiveness to the topic, our system ranked third out of 35 systems. &#211; 2007 Elsevier Ltd. All rights reserved. </abstract><biblio /></article>
<article><preamble>Alexandrov_2015_A_Modified_Tripartite_Model_for_Document_Representation_in_Internet_Sociology</preamble><titre>A Modified Tripartite Model for Document</titre><auteur>Unknown</auteur><abstract> Seven years ago Peter Mika (Yahoo! Research) proposed a tripartite model of actors, concepts and instances for document representation in the study of social networks. We propose a modified model, where instead of document authors we consider textual mentions of persons and institutions as actors. This representation proves to be more appropriate for the solution of a range of Internet Sociology tasks. In the paper we describe experiments with the modified model and provide some background on the tools that can be used to build it. The model is tested on the experimental corpora of Russian news (educational domain). The research reflects the pilot study findings. </abstract><biblio /></article>
<article><preamble>Doyle_2005_Automatic_Categorization_of_Author_Gender</preamble><titre>Automatic Categorization of Author Gender</titre><auteur>Unknown</auteur><abstract> We present a method for automatic categorization of author gender via n-gram analysis. Using a corpus of British student essays, experiments using character-level, wordlevel, and part-of-speech n-grams are performed. The peak accuracy for all methods is roughly equal, reaching a maximum of 81%. These results are on par with other, established techniques, while retaining the simplicity and ease-of-generalization inherent in n-gram techniques.</abstract><biblio /></article>
<article><preamble>Furui_2004_Speech-to-text_and_speech-to-speech_summarization_of_spontaneous_speech</preamble><titre>IEEE TRANSACTIONS ON SPEECH AND AUDIO PROCESSING, VOL. 12, NO. 4, JULY 2004</titre><auteur>Unknown</auteur><abstract>This paper presents techniques for speech-to-text and speech-to-speech automatic summarization based on speech unit extraction and concatenation. For the former case, a two-stage summarization method consisting of important sentence extraction and word-based sentence compaction is investigated. Sentence and word units which maximize the weighted sum of linguistic likelihood, amount of information, confidence measure, and grammatical likelihood of concatenated units are extracted from the speech recognition results and concatenated for producing summaries. For the latter case, sentences, words, and between-filler units are investigated as units to be extracted from original speech. These methods are applied to the summarization of unrestricted-domain spontaneous presentations and evaluated by objective and subjective measures. It was confirmed that proposed methods are effective in spontaneous speech summarization. Index Terms&#8212;Presentation, speech recognition, speech summarization, speech-to-speech, speech-to-text, spontaneous speech.</abstract><biblio /></article>
<article><preamble>Gonzalez_2018_Automated_Sentence_Boundary_Detection_in_Modern_StandardArabic_Transcripts_using_Deep_Neural_Networks</preamble><titre>Available</titre><auteur>Unknown</auteur><abstract> </abstract><biblio /></article>
<article><preamble>Levner_2007_Fuzzifying_clustering_algorithms_The_case_study_of_MajorClust</preamble><titre>Fuzzifying clustering algorithms:</titre><auteur>Unknown</auteur><abstract> Among various document clustering algorithms that have been proposed so far, the most useful are those that automatically reveal the number of clusters and assign each target document to exactly one cluster. However, in many real situations, there not exists an exact boundary between different clusters. In this work, we introduce a fuzzy version of the MajorClust algorithm. The proposed clustering method assigns documents to more than one category by taking into account a membership function for both, edges and nodes of the corresponding underlying graph. Thus, the clustering problem is formulated in terms of weighted fuzzy graphs. The fuzzy approach permits to decrease some negative effects which appear in clustering of large-sized corpora with noisy data.</abstract><biblio /></article>
<article><preamble>Lin_2004_Rouge</preamble><titre>ROUGE: A Package for Automatic Evaluation of Summaries</titre><auteur>Unknown</auteur><abstract> ROUGE stands for Recall-Oriented Understudy for Gisting Evaluation. It includes measures to automatically determine the quality of a summary by comparing it to other (ideal) summaries created by humans. The measures count the number of overlapping units such as n-gram, word sequences, and word pairs between the computer-generated summary to be evaluated and the ideal summaries created by humans. This paper introduces four different ROUGE measures: ROUGE-N, ROUGE-L, ROUGE-W, and ROUGE-S included in the ROUGE summarization evaluation package and their evaluatio ns. Three of them have been used in the Document Understanding Conference (DUC) 2004, a large-scale summarization evaluation sponsored by NIST. 1</abstract><biblio /></article>
<article><preamble>Metsis_2006_Spam_filtering_with_naive_bayes-which_naive_bayes</preamble><titre>Spam Filtering with Naive Bayes &#8211; Which Naive Bayes?</titre><auteur>Unknown</auteur><abstract> Naive Bayes is very popular in commercial and open-source anti-spam e-mail filters. There are, however, several forms of Naive Bayes, something the anti-spam literature does not always acknowledge. We discuss five different versions of Naive Bayes, and compare them on six new, non-encoded datasets, that contain ham messages of particular Enron users and fresh spam messages. The new datasets, which we make publicly available, are more realistic than previous comparable benchmarks, because they maintain the temporal order of the messages in the two categories, and they emulate the varying proportion of spam and ham messages that users receive over time. We adopt an experimental procedure that emulates the incremental training of personalized spam filters, and we plot roc curves that allow us to compare the different versions of nb over the entire tradeoff between true positives and true negatives.</abstract><biblio /></article>
<article><preamble>Mikolov_2013_Distributed_representations_of_words_and_phrases_and_their_compositionality</preamble><titre>Distributed Representations of Words and Phrases</titre><auteur>Unknown</auteur><abstract> The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of &#8220;Canada&#8221; and &#8220;Air&#8221; cannot be easily combined to obtain &#8220;Air Canada&#8221;. Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.</abstract><biblio /></article>
<article><preamble>Torres-Moreno_2012_Artex_is_another_text_summarizer</preamble><titre>Artex is AnotheR TEXt summarizer</titre><auteur>Unknown</auteur><abstract> This paper describes Artex, another algorithm for Automatic Text Summarization. In order to rank sentences, a simple inner product is calculated between each sentence, a document vector (text topic) and a lexical vector (vocabulary used by a sentence). Summaries are then generated by assembling the highest ranked sentences. No ruled-based linguistic post-processing is necessary in order to obtain summaries. Tests over several datasets (coming from Document Understanding Conferences (DUC), Text Analysis Conference (TAC), evaluation campaigns, etc.) in French, English and Spanish have shown that Artex summarizer achieves interesting results.</abstract><biblio /></article>
<article><preamble>Vanderwende_2007_Beyond_SumBasic</preamble><titre>Information Processing and Management 43 (2007) 1606&#8211;1618</titre><auteur>Unknown</auteur><abstract> In recent years, there has been increased interest in topic-focused multi-document summarization. In this task, automatic summaries are produced in response to a speci&#64257;c information request, or topic, stated by the user. The system we have designed to accomplish this task comprises four main components: a generic extractive summarization system, a topic-focusing component, sentence simpli&#64257;cation, and lexical expansion of topic words. This paper details each of these components, together with experiments designed to quantify their individual contributions. We include an analysis of our results on two large datasets commonly used to evaluate task-focused summarization, the DUC2005 and DUC2006 datasets, using automatic metrics. Additionally, we include an analysis of our results on the DUC2006 task according to human evaluation metrics. In the human evaluation of system summaries compared to human summaries, i.e., the Pyramid method, our system ranked &#64257;rst out of 22 systems in terms of overall mean Pyramid score; and in the human evaluation of summary responsiveness to the topic, our system ranked third out of 35 systems. &#211; 2007 Elsevier Ltd. All rights reserved. </abstract><biblio /></article>
<article><preamble>Alexandrov_2015_A_Modified_Tripartite_Model_for_Document_Representation_in_Internet_Sociology</preamble><titre>A Modified Tripartite Model for Document</titre><auteur>Unknown</auteur><abstract> Seven years ago Peter Mika (Yahoo! Research) proposed a tripartite model of actors, concepts and instances for document representation in the study of social networks. We propose a modified model, where instead of document authors we consider textual mentions of persons and institutions as actors. This representation proves to be more appropriate for the solution of a range of Internet Sociology tasks. In the paper we describe experiments with the modified model and provide some background on the tools that can be used to build it. The model is tested on the experimental corpora of Russian news (educational domain). The research reflects the pilot study findings. </abstract><biblio /></article>
<article><preamble>Doyle_2005_Automatic_Categorization_of_Author_Gender</preamble><titre>Automatic Categorization of Author Gender</titre><auteur>Unknown</auteur><abstract> We present a method for automatic categorization of author gender via n-gram analysis. Using a corpus of British student essays, experiments using character-level, wordlevel, and part-of-speech n-grams are performed. The peak accuracy for all methods is roughly equal, reaching a maximum of 81%. These results are on par with other, established techniques, while retaining the simplicity and ease-of-generalization inherent in n-gram techniques.</abstract><biblio /></article>
<article><preamble>Furui_2004_Speech-to-text_and_speech-to-speech_summarization_of_spontaneous_speech</preamble><titre>IEEE TRANSACTIONS ON SPEECH AND AUDIO PROCESSING, VOL. 12, NO. 4, JULY 2004</titre><auteur>Unknown</auteur><abstract>This paper presents techniques for speech-to-text and speech-to-speech automatic summarization based on speech unit extraction and concatenation. For the former case, a two-stage summarization method consisting of important sentence extraction and word-based sentence compaction is investigated. Sentence and word units which maximize the weighted sum of linguistic likelihood, amount of information, confidence measure, and grammatical likelihood of concatenated units are extracted from the speech recognition results and concatenated for producing summaries. For the latter case, sentences, words, and between-filler units are investigated as units to be extracted from original speech. These methods are applied to the summarization of unrestricted-domain spontaneous presentations and evaluated by objective and subjective measures. It was confirmed that proposed methods are effective in spontaneous speech summarization. Index Terms&#8212;Presentation, speech recognition, speech summarization, speech-to-speech, speech-to-text, spontaneous speech.</abstract><biblio /></article>
<article><preamble>Gonzalez_2018_Automated_Sentence_Boundary_Detection_in_Modern_StandardArabic_Transcripts_using_Deep_Neural_Networks</preamble><titre>Available</titre><auteur>Unknown</auteur><abstract> </abstract><biblio /></article>
<article><preamble>Levner_2007_Fuzzifying_clustering_algorithms_The_case_study_of_MajorClust</preamble><titre>Fuzzifying clustering algorithms:</titre><auteur>Unknown</auteur><abstract> Among various document clustering algorithms that have been proposed so far, the most useful are those that automatically reveal the number of clusters and assign each target document to exactly one cluster. However, in many real situations, there not exists an exact boundary between different clusters. In this work, we introduce a fuzzy version of the MajorClust algorithm. The proposed clustering method assigns documents to more than one category by taking into account a membership function for both, edges and nodes of the corresponding underlying graph. Thus, the clustering problem is formulated in terms of weighted fuzzy graphs. The fuzzy approach permits to decrease some negative effects which appear in clustering of large-sized corpora with noisy data.</abstract><biblio /></article>
<article><preamble>Lin_2004_Rouge</preamble><titre>ROUGE: A Package for Automatic Evaluation of Summaries</titre><auteur>Unknown</auteur><abstract> ROUGE stands for Recall-Oriented Understudy for Gisting Evaluation. It includes measures to automatically determine the quality of a summary by comparing it to other (ideal) summaries created by humans. The measures count the number of overlapping units such as n-gram, word sequences, and word pairs between the computer-generated summary to be evaluated and the ideal summaries created by humans. This paper introduces four different ROUGE measures: ROUGE-N, ROUGE-L, ROUGE-W, and ROUGE-S included in the ROUGE summarization evaluation package and their evaluatio ns. Three of them have been used in the Document Understanding Conference (DUC) 2004, a large-scale summarization evaluation sponsored by NIST. 1</abstract><biblio /></article>
<article><preamble>Metsis_2006_Spam_filtering_with_naive_bayes-which_naive_bayes</preamble><titre>Spam Filtering with Naive Bayes &#8211; Which Naive Bayes?</titre><auteur>Unknown</auteur><abstract> Naive Bayes is very popular in commercial and open-source anti-spam e-mail filters. There are, however, several forms of Naive Bayes, something the anti-spam literature does not always acknowledge. We discuss five different versions of Naive Bayes, and compare them on six new, non-encoded datasets, that contain ham messages of particular Enron users and fresh spam messages. The new datasets, which we make publicly available, are more realistic than previous comparable benchmarks, because they maintain the temporal order of the messages in the two categories, and they emulate the varying proportion of spam and ham messages that users receive over time. We adopt an experimental procedure that emulates the incremental training of personalized spam filters, and we plot roc curves that allow us to compare the different versions of nb over the entire tradeoff between true positives and true negatives.</abstract><biblio /></article>
<article><preamble>Mikolov_2013_Distributed_representations_of_words_and_phrases_and_their_compositionality</preamble><titre>Distributed Representations of Words and Phrases</titre><auteur>Unknown</auteur><abstract> The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of &#8220;Canada&#8221; and &#8220;Air&#8221; cannot be easily combined to obtain &#8220;Air Canada&#8221;. Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.</abstract><biblio /></article>
<article><preamble>Torres-Moreno_2012_Artex_is_another_text_summarizer</preamble><titre>Artex is AnotheR TEXt summarizer</titre><auteur>Unknown</auteur><abstract> This paper describes Artex, another algorithm for Automatic Text Summarization. In order to rank sentences, a simple inner product is calculated between each sentence, a document vector (text topic) and a lexical vector (vocabulary used by a sentence). Summaries are then generated by assembling the highest ranked sentences. No ruled-based linguistic post-processing is necessary in order to obtain summaries. Tests over several datasets (coming from Document Understanding Conferences (DUC), Text Analysis Conference (TAC), evaluation campaigns, etc.) in French, English and Spanish have shown that Artex summarizer achieves interesting results.</abstract><biblio /></article>
<article><preamble>Vanderwende_2007_Beyond_SumBasic</preamble><titre>Information Processing and Management 43 (2007) 1606&#8211;1618</titre><auteur>Unknown</auteur><abstract> In recent years, there has been increased interest in topic-focused multi-document summarization. In this task, automatic summaries are produced in response to a speci&#64257;c information request, or topic, stated by the user. The system we have designed to accomplish this task comprises four main components: a generic extractive summarization system, a topic-focusing component, sentence simpli&#64257;cation, and lexical expansion of topic words. This paper details each of these components, together with experiments designed to quantify their individual contributions. We include an analysis of our results on two large datasets commonly used to evaluate task-focused summarization, the DUC2005 and DUC2006 datasets, using automatic metrics. Additionally, we include an analysis of our results on the DUC2006 task according to human evaluation metrics. In the human evaluation of system summaries compared to human summaries, i.e., the Pyramid method, our system ranked &#64257;rst out of 22 systems in terms of overall mean Pyramid score; and in the human evaluation of summary responsiveness to the topic, our system ranked third out of 35 systems. &#211; 2007 Elsevier Ltd. All rights reserved. </abstract><biblio /></article>
